{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "32baf884",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import boto3\n",
    "import pandas as pd\n",
    "\n",
    "def read_datasets():\n",
    "    # Read three datasets stored in the Amazon S3 bucket\n",
    "   # bucket = \"faostat-ml\"\n",
    "    file_name = \"Emissions_Totals_E_All_Data_(Normalized).csv\"\n",
    "    #s3_client = boto3.client(\"s3\")\n",
    "    #obj = s3_client.get_object(Bucket=bucket, Key=file_name)\n",
    "    df_emission = pd.read_csv(\"Emissions_Totals_E_All_Data_(Normalized).csv\",encoding='latin-1')\n",
    "    df_emission.drop(df_emission.columns[df_emission.columns.str.contains('unnamed',case = False)],axis = 1, inplace = True)\n",
    "\n",
    "    #fix the feature names\n",
    "    df_emission = df_emission.rename(columns={'Item': 'EmissionItem','Value': 'EmissionValue','Element': 'EmissionElement','Unit': 'EmissionUnit'})\n",
    "    print(df_emission.head(5))\n",
    "\n",
    "    file_name = \"Production_Crops_Livestock_E_All_Data_(Normalized).csv\"\n",
    "    #obj = s3_client.get_object(Bucket=bucket, Key=file_name)\n",
    "    df_prod = pd.read_csv(\"Production_Crops_Livestock_E_All_Data_(Normalized).csv\",encoding='latin-1')\n",
    "    df_prod.drop(df_prod.columns[df_prod.columns.str.contains('unnamed',case = False)],axis = 1, inplace = True)\n",
    "    df_prod.drop('Unit', axis=1, inplace=True)\n",
    "    print(df_prod.head(5))\n",
    "\n",
    "\n",
    "    file_name = \"Forestry_E_All_Data_(Normalized).csv\"\n",
    "    #obj = s3_client.get_object(Bucket=bucket, Key=file_name)\n",
    "    df_forest = pd.read_csv(\"Forestry_E_All_Data_(Normalized).csv\",encoding='latin-1')\n",
    "    df_forest.drop(df_forest.columns[df_forest.columns.str.contains('unnamed',case = False)],axis = 1, inplace = True)\n",
    "    print(df_forest.head(5))\n",
    "    return df_emission, df_prod, df_forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ad8d11d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing():\n",
    "    remove_rows_df = pre_process_df.copy()\n",
    "    \n",
    "    # Removes area, source and emission unit that place no significant role\n",
    "    remove_rows_df.drop('Area', axis=1, inplace=True)\n",
    "    remove_rows_df.drop('Source', axis=1, inplace=True)\n",
    "    remove_rows_df.drop('EmissionUnit', axis=1, inplace=True)\n",
    "    \n",
    "    # Remove redudant instance\n",
    "    remove_rows_df = remove_rows_df[remove_rows_df.Element != \"Area harvested\"]\n",
    "    remove_rows_df = remove_rows_df[remove_rows_df.Element != \"Production\"]\n",
    "    remove_rows_df = remove_rows_df[remove_rows_df.Element != \"Producing Animals/Slaughtered\"]\n",
    "    remove_rows_df = remove_rows_df[remove_rows_df.EmissionElement != \"Indirect emissions (N2O)\"]\n",
    "    remove_rows_df = remove_rows_df[remove_rows_df.EmissionElement != \"Direct emissions (N2O)\"]\n",
    "    remove_rows_df.index = remove_rows_df.Year\n",
    "    remove_rows_df.drop('Year', axis=1, inplace=True)\n",
    "\n",
    "    #identify partial string to look for\n",
    "    discard = ['from']\n",
    "    remove_rows_df = remove_rows_df[~remove_rows_df.EmissionElement.str.contains('|'.join(discard))]\n",
    "\n",
    "    # Combine two features\n",
    "    remove_rows_df[\"Emission\"] = remove_rows_df[\"EmissionItem\"] + str(\"_\") + remove_rows_df[\"EmissionElement\"]\n",
    "    remove_rows_df.drop('EmissionItem', axis=1, inplace=True)\n",
    "    remove_rows_df.drop('EmissionElement', axis=1, inplace=True)\n",
    "\n",
    "    # Remove redudant instance\n",
    "    remove_rows_df = remove_rows_df[remove_rows_df.Element != 'Import Value']\n",
    "    remove_rows_df = remove_rows_df[remove_rows_df.Element != 'Export Value']\n",
    "    \n",
    "    emission_list = list(pre_process_df.EmissionItem.unique())\n",
    "\n",
    "    # Create pivot table for production items + forestry products based on year and item\n",
    "    df_item = remove_rows_df.pivot_table(index=['Year'], \n",
    "                columns=['Item'], values='Value')\n",
    "    \n",
    "    # Remove columns that have atleast one NaN value since it would affect the forecast\n",
    "    sum = df_item.isnull().sum(axis = 0)\n",
    "    for items in sum.iteritems():\n",
    "        if(items[1]>0):\n",
    "            df_item.drop(items[0], axis=1, inplace=True)\n",
    "    nan_cols = [i for i in df_item.columns if df_item[i].isnull().any()]\n",
    "\n",
    "    # Create pivot table for emissions\n",
    "    df_emi = remove_rows_df.pivot_table(index=['Year'], \n",
    "                columns=['Emission'], values='EmissionValue')\n",
    "    # Remove columns that have atleast one NaN value since it would affect the forecast\n",
    "    sum = df_emi.isnull().sum(axis = 0)\n",
    "    for items in sum.iteritems():\n",
    "        if(items[1]>0):\n",
    "            df_emi.drop(items[0], axis=1, inplace=True)\n",
    "    nan_cols = [i for i in df_emi.columns if df_emi[i].isnull().any()]\n",
    "    \n",
    "    display(df_item.head(5))\n",
    "    display(df_emi.head(5))\n",
    "    return df_item, df_emi, emission_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6a270313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform granger causation test - to check the influence of one variable over another\n",
    "\n",
    "def grangers_causation_matrix(data, variables, test='ssr_chi2test', verbose=False):    \n",
    "    import numpy as np\n",
    "    from statsmodels.tsa.stattools import grangercausalitytests\n",
    "    maxlag=12\n",
    "    \"\"\"Check Granger Causality of all possible combinations of the Time series.\n",
    "    The rows are the response variable, columns are predictors. The values in the table \n",
    "    are the P-Values. P-Values lesser than the significance level (0.05), implies \n",
    "    the Null Hypothesis that the coefficients of the corresponding past values is \n",
    "    zero, that is, the X does not cause Y can be rejected.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(np.zeros((len(variables), len(variables))), columns=variables, index=variables)\n",
    "    for c in df.columns:\n",
    "        for r in df.index:\n",
    "            test_result = grangercausalitytests(data[[r, c]], maxlag=maxlag, verbose=False)\n",
    "            p_values = [round(test_result[i+1][0][test][1],4) for i in range(maxlag)]\n",
    "            if verbose: print(f'Y = {r}, X = {c}, P Values = {p_values}')\n",
    "            min_p_value = np.min(p_values)\n",
    "            df.loc[r, c] = min_p_value\n",
    "    df.columns = [var + '_x' for var in variables]\n",
    "    df.index = [var + '_y' for var in variables]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c02293be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform ADF test to check if each series is stationary or not\n",
    "\n",
    "def adfuller_test(series, signif=0.05, name='', verbose=False):\n",
    "    i = 0\n",
    "    from statsmodels.tsa.api import VECM\n",
    "    import semopy\n",
    "    from statsmodels.tsa.stattools import adfuller\n",
    "    from statsmodels.tools.eval_measures import rmse, aic\n",
    "    non_stationary = []\n",
    "    \"\"\"Perform ADFuller to test for Stationarity of given series and print report\"\"\"\n",
    "    r = adfuller(series, autolag='AIC')\n",
    "    output = {'test_statistic':round(r[0], 4), 'pvalue':round(r[1], 4), 'n_lags':round(r[2], 4), 'n_obs':r[3]}\n",
    "    p_value = output['pvalue'] \n",
    "    def adjust(val, length= 6): return str(val).ljust(length)\n",
    "\n",
    "    # Print Summary\n",
    "    print(f'    Augmented Dickey-Fuller Test on \"{name}\"', \"\\n   \", '-'*47)\n",
    "    print(f' Null Hypothesis: Data has unit root. Non-Stationary.')\n",
    "    print(f' Significance Level    = {signif}')\n",
    "    print(f' Test Statistic        = {output[\"test_statistic\"]}')\n",
    "    print(f' No. Lags Chosen       = {output[\"n_lags\"]}')\n",
    "    \n",
    "    for key,val in r[4].items():\n",
    "        i = i + 1\n",
    "        print(f' Critical value {adjust(key)} = {round(val, 3)}')\n",
    "\n",
    "    if p_value <= signif:\n",
    "        print(f\" => P-Value = {p_value}. Rejecting Null Hypothesis.\")\n",
    "        print(f\" => Series is Stationary.\")\n",
    "    else:\n",
    "        print(f\" => P-Value = {p_value}. Weak evidence to reject the Null Hypothesis.\")\n",
    "        print(f\" => Series is Non-Stationary.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d156612b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Revert back the differencing to get the forecast to original scale\n",
    "\n",
    "def invert_transformation(df_train, df_forecast, second_diff=False):\n",
    "    df_fc = df_forecast.copy()\n",
    "    columns = df_train.columns\n",
    "    for col in columns:        \n",
    "        # Roll back 2nd Diff\n",
    "        if second_diff:\n",
    "            df_fc[str(col)+'_1d'] = (df_train[col].iloc[-1]-df_train[col].iloc[-2]) + df_fc[str(col)+'_2d'].cumsum()\n",
    "        # Roll back 1st Diff\n",
    "        df_fc[str(col)+'_forecast'] = df_train[col].iloc[-1] + df_fc[str(col)+'_1d'].cumsum()\n",
    "    return df_fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4a799e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform stationary test and differencing\n",
    "\n",
    "\n",
    "\"\"\" Note: Comment out the 'for loop' when adf fuller test results need to be displayed\"\"\"\n",
    "\n",
    "def stationary(df):\n",
    "    nobs = 9\n",
    "    df_train, df_test = df[0:-nobs], df[-nobs:]\n",
    "    \n",
    "    # ADF Test on each column\n",
    "    #for name, column in df_train.iteritems():\n",
    "        #adfuller_test(column, name=column.name)\n",
    "        #print('\\n')\n",
    "        \n",
    "    # 1st difference\n",
    "    df_differenced = df_train.diff().dropna()\n",
    "    #for name, column in df_differenced.iteritems():\n",
    "        #adfuller_test(column, name=column.name)\n",
    "        #print('\\n')\n",
    "        \n",
    "    # Second Differencing\n",
    "    df_differenced = df_differenced.diff().dropna()   \n",
    "    #for name, column in df_differenced.iteritems():\n",
    "        #adfuller_test(column, name=column.name)\n",
    "        #print('\\n')\n",
    "        \n",
    "    return df_differenced, df_train, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "50711a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function that performs forecasting\n",
    "\n",
    "mae =[]\n",
    "mse = []\n",
    "sqr = []\n",
    "    \n",
    "def forecasting(df_item, df_emi, emission_list):\n",
    "    import warnings\n",
    "    import pandas as pd\n",
    "    from numpy import sqrt \n",
    "    import plotly.graph_objs as go\n",
    "    from statsmodels.tsa.api import VAR\n",
    "    from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "    warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "    col = df_item.columns\n",
    "    pandas_df = pd.DataFrame()\n",
    "    nobs = 9\n",
    "\n",
    "    # iterate through every item - production and forestry product - for the country/area specified\n",
    "    for i in col:\n",
    "        selected_columns = df_item[i]\n",
    "\n",
    "        new_df = selected_columns.copy()\n",
    "        new_merged_df = pd.merge(new_df, df_emi, on=['Year'])\n",
    "\n",
    "        for j in emission_list:\n",
    "            emission = new_merged_df.filter(regex=j)\n",
    "            col = list(emission.columns) \n",
    "            new_merged_df[j] = new_merged_df[col].sum(axis=1)\n",
    "\n",
    "        pandas_df = new_merged_df[new_merged_df.columns.intersection(emission_list)]\n",
    "        pandas_df[str(new_merged_df.columns[0])] = new_merged_df[str(new_merged_df.columns[0])]\n",
    "        pandas_df = pandas_df.loc[:, (pandas_df != 0).any(axis=0)]\n",
    "         \n",
    "        \"\"\" Note: comment out if granger test needs to be performed\"\"\"\n",
    "        #print(\"Granger's causality test:\")\n",
    "        #display(grangers_causation_matrix(pandas_df, variables = pandas_df.columns))        \n",
    "        \n",
    "        historical_split1 = pandas_df.iloc[:50,:]\n",
    "        historical_split2 = pandas_df.iloc[50:,:]\n",
    "    \n",
    "        df_differenced,df_train,_ = stationary(historical_split1)\n",
    "        model = VAR(df_differenced)\n",
    "        model_fitted = model.fit(6)\n",
    "\n",
    "        # Get the lag order\n",
    "        lag_order = model_fitted.k_ar\n",
    "\n",
    "        # Input data for forecasting\n",
    "        forecast_input = df_differenced.values[-lag_order:]\n",
    "        fc = model_fitted.forecast(y=forecast_input, steps=nobs)\n",
    "        df_forecast = pd.DataFrame(fc, index=historical_split1.index[-nobs:], columns=historical_split1.columns + '_2d')\n",
    "\n",
    "        df_results = invert_transformation(df_train, df_forecast, second_diff=True)\n",
    "        d = historical_split1.tail(nobs)\n",
    "        d.reset_index(inplace = True)\n",
    "\n",
    "\n",
    "        range_year = pd.date_range(start = str(d.Year.iloc[-1]), periods = (len(d)+1), freq = 'A')\n",
    "        range_year = range_year.year\n",
    "        year = pd.DataFrame({'Year': range_year})\n",
    "        d = d.append(year)\n",
    "        year_forecast = d['Year'].iat[-1]\n",
    "\n",
    "        d.set_index('Year', inplace = True)\n",
    "        d = d.tail(nobs)\n",
    "            \n",
    "        df_results.index = d.index\n",
    "        mean_old = historical_split1[i].mean()\n",
    "        recent_forecast = df_results[i+\"_forecast\"]\n",
    "        recent_forecast = recent_forecast.iat[-1]\n",
    "\n",
    "        #combining predicted and real data set\n",
    "        combine = pd.concat([df_results[i+\"_forecast\"], historical_split2[i]], axis=1)\n",
    "        combine = combine.round(decimals=2)\n",
    "        combine = combine.reset_index()\n",
    "\n",
    "        combine[i+\"_Unscaled\"] = combine[i]\n",
    "        combine[i+ \"_Forecast_Unscaled\"] = combine[i+\"_forecast\"]\n",
    "\n",
    "        combine[i]=(combine[i]-combine[i].min())/(combine[i].max()-combine[i].min())\n",
    "        combine[i+\"_forecast\"]=(combine[i+\"_forecast\"]-combine[i+\"_forecast\"].min())/(combine[i+\"_forecast\"].max()-combine[i+\"_forecast\"].min())\n",
    "\n",
    "\n",
    "        display(combine)\n",
    "        null_check = combine.isnull().sum().sum()\n",
    "        if null_check > 0:\n",
    "            continue\n",
    "        #Forecast metrics\n",
    "\n",
    "        mae.append(mean_absolute_error(combine[i].values, combine[i+\"_forecast\"].values))\n",
    "        mse.append(mean_squared_error(combine[i].values, combine[i+\"_forecast\"].values))\n",
    "        sqr.append(sqrt(mean_squared_error(combine[i].values, combine[i+\"_forecast\"].values)))\n",
    "        \n",
    "        fig = go.Figure()\n",
    "        n = df_results.index[0]\n",
    "        fig.add_trace(go.Scatter(x = pandas_df.index[-200:], y = pandas_df[str(i)][-200:], marker = dict(color =\"red\"), name = \"Actual close price\"))\n",
    "        fig.add_trace(go.Scatter(x = df_results.index, y = df_results[str(i)+'_forecast'], marker=dict(color = \"green\"), name = \"Future prediction\"))\n",
    "        fig.update_xaxes(showline = True, linewidth = 2, linecolor='black', mirror = True, showspikes = True,)\n",
    "        fig.update_yaxes(showline = True, linewidth = 2, linecolor='black', mirror = True, showspikes = True,)\n",
    "        fig.update_layout(title= \"9 Years Forecast\", yaxis_title = str(i), hovermode = \"x\", hoverdistance = 100) #, # Distance to show hover label of data point spikedistance = 1000,shapes = [dict( x0 = n, x1 = n, y0 = 0, y1 = 1, xref = 'x', yref = 'paper', line_width = 2)], annotations = [dict(x = n, y = 0.05, xref = 'x', yref = 'paper', showarrow = False, xanchor = 'left', text = 'Prediction')])\n",
    "        fig.update_layout(autosize = False, width = 1000, height = 400,)\n",
    "        fig.show()\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1e7a236b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forecasting1(df_item, df_emi, emission_list):\n",
    "    import warnings\n",
    "    import pandas as pd\n",
    "    from numpy import sqrt \n",
    "    import plotly.graph_objs as go\n",
    "    from statsmodels.tsa.api import VAR\n",
    "    from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "    from statsmodels.stats.stattools import durbin_watson\n",
    "\n",
    "    warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "    col = df_item.columns\n",
    "    pandas_df = pd.DataFrame()\n",
    "    nobs = 9\n",
    "\n",
    "    # iterate through every item - production and forestry product - for the country/area specified\n",
    "    for i in col:\n",
    "        selected_columns = df_item[i]\n",
    "\n",
    "        new_df = selected_columns.copy()\n",
    "        new_merged_df = pd.merge(new_df, df_emi, on=['Year'])\n",
    "\n",
    "        for j in emission_list:\n",
    "            emission = new_merged_df.filter(regex=j)\n",
    "            col = list(emission.columns) \n",
    "            new_merged_df[j] = new_merged_df[col].sum(axis=1)\n",
    "\n",
    "        pandas_df = new_merged_df[new_merged_df.columns.intersection(emission_list)]\n",
    "        pandas_df[str(new_merged_df.columns[0])] = new_merged_df[str(new_merged_df.columns[0])]\n",
    "        pandas_df = pandas_df.loc[:, (pandas_df != 0).any(axis=0)]\n",
    "         \n",
    "        \"\"\" Note: comment out if granger test needs to be performed\"\"\"\n",
    "        #print(\"Granger's causality test:\")\n",
    "        #display(grangers_causation_matrix(pandas_df, variables = pandas_df.columns))        \n",
    "        \n",
    "        historical_split1 = pandas_df.iloc[:50,:]\n",
    "        historical_split2 = pandas_df.iloc[50:,:]\n",
    "    \n",
    "        df_differenced,df_train,_ = stationary(historical_split1)\n",
    "        model = VAR(df_differenced)\n",
    "        model_fitted = model.fit(6)\n",
    "        \n",
    "        results = model.fit(maxlags=2, ic='aic')\n",
    "\n",
    "# make predictions for the next 10 periods\n",
    "        lag_order = results.k_ar\n",
    "        predictions = results.forecast(df_differenced.values[-lag_order:], steps=10)\n",
    "\n",
    "        # calculate the residuals\n",
    "        residuals = df_differenced.values[-10:] - predictions\n",
    "\n",
    "        # plot the residuals\n",
    "        plt.plot(residuals)\n",
    "        plt.title('Residuals')\n",
    "        plt.show()\n",
    "\n",
    "        # calculate the Durbin-Watson statistic for each variable\n",
    "        for i, col in enumerate(df_differenced.columns):\n",
    "            dw = durbin_watson(residuals[:, i])\n",
    "            print(f'Durbin-Watson statistic for {col}: {dw}')\n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fb2c619a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_series_prediction():\n",
    "    pre_process_df = pd.DataFrame()\n",
    "    combined_df = pd.DataFrame()\n",
    "\n",
    "    import numpy as np\n",
    "    import seaborn as sb\n",
    "    import ipywidgets as widgets\n",
    "    import matplotlib.pyplot as plt\n",
    "    from sklearn import preprocessing\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    from ipywidgets import Layout, Button, Box, FloatText, Textarea, Dropdown, Label, IntSlider\n",
    "\n",
    "    def unique_sorted_values_plus_ALL(array):\n",
    "        unique = array.unique().tolist()\n",
    "        unique.sort()\n",
    "        return unique\n",
    "    output_area = widgets.Output()\n",
    "    print(\"Area:\")\n",
    "    dropdown_area = widgets.Dropdown(options=unique_sorted_values_plus_ALL(df_prod.Area))\n",
    "    def dropdown_area_eventhandler(change):\n",
    "        output_area.clear_output()\n",
    "        with output_area:\n",
    "            if(change.new):\n",
    "                country_df_prod = df_prod[df_prod.Area == change.new]\n",
    "                country_df_forest = df_forest[df_forest.Area == change.new]\n",
    "                country_df_emission = df_emission[df_emission.Area == change.new]\n",
    "\n",
    "                prod_forest_df = pd.concat([country_df_prod, country_df_forest], ignore_index=True)\n",
    "                global pre_process_df, combined_df\n",
    "                pre_process_df = pd.merge(prod_forest_df, country_df_emission, on=['Year','Area'])\n",
    "    dropdown_area.observe(dropdown_area_eventhandler, names='value')\n",
    "    display(dropdown_area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "06cd5ef4",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Emissions_Totals_E_All_Data_(Normalized).csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Run the functions from this module onwards for time series prediction\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m df_emission, df_prod, df_forest \u001b[38;5;241m=\u001b[39m \u001b[43mread_datasets\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[13], line 11\u001b[0m, in \u001b[0;36mread_datasets\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m file_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEmissions_Totals_E_All_Data_(Normalized).csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m#s3_client = boto3.client(\"s3\")\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m#obj = s3_client.get_object(Bucket=bucket, Key=file_name)\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m df_emission \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEmissions_Totals_E_All_Data_(Normalized).csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlatin-1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m df_emission\u001b[38;5;241m.\u001b[39mdrop(df_emission\u001b[38;5;241m.\u001b[39mcolumns[df_emission\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mcontains(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124munnamed\u001b[39m\u001b[38;5;124m'\u001b[39m,case \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m)],axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m, inplace \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m#fix the feature names\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m    947\u001b[0m )\n\u001b[0;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    602\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    604\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 605\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1439\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1733\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1734\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1736\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1737\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1738\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1739\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1740\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1741\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1742\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1743\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1744\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    851\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    852\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    853\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    854\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    855\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 856\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    857\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    858\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    859\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    862\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    863\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    864\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    865\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Emissions_Totals_E_All_Data_(Normalized).csv'"
     ]
    }
   ],
   "source": [
    "# Run the functions from this module onwards for time series prediction\n",
    "\n",
    "df_emission, df_prod, df_forest = read_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "973de5c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df_prod' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtime_series_prediction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[21], line 19\u001b[0m, in \u001b[0;36mtime_series_prediction\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m output_area \u001b[38;5;241m=\u001b[39m widgets\u001b[38;5;241m.\u001b[39mOutput()\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArea:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 19\u001b[0m dropdown_area \u001b[38;5;241m=\u001b[39m widgets\u001b[38;5;241m.\u001b[39mDropdown(options\u001b[38;5;241m=\u001b[39munique_sorted_values_plus_ALL(\u001b[43mdf_prod\u001b[49m\u001b[38;5;241m.\u001b[39mArea))\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdropdown_area_eventhandler\u001b[39m(change):\n\u001b[0;32m     21\u001b[0m     output_area\u001b[38;5;241m.\u001b[39mclear_output()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_prod' is not defined"
     ]
    }
   ],
   "source": [
    "time_series_prediction()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dfacdbb",
   "metadata": {},
   "source": [
    "The below cell outputs consist of Granger causality matrix, ADF fuller test results for each feature and prediction graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3486618f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pre_process_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDelftStack\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDo not show this message\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m display(\u001b[43mpre_process_df\u001b[49m\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m5\u001b[39m))\n\u001b[0;32m      7\u001b[0m df_item, df_emi, emission_list \u001b[38;5;241m=\u001b[39m pre_processing()\n\u001b[0;32m      8\u001b[0m mae,mse,sqr \u001b[38;5;241m=\u001b[39m forecasting(df_item, df_emi, emission_list)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pre_process_df' is not defined"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.warn('DelftStack')\n",
    "warnings.warn('Do not show this message')\n",
    "\n",
    "display(pre_process_df.head(5))\n",
    "df_item, df_emi, emission_list = pre_processing()\n",
    "mae,mse,sqr = forecasting(df_item, df_emi, emission_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "378bd8c7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pre_process_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDelftStack\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      9\u001b[0m warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDo not show this message\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 11\u001b[0m display(\u001b[43mpre_process_df\u001b[49m\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m5\u001b[39m))\n\u001b[0;32m     12\u001b[0m df_item, df_emi, emission_list \u001b[38;5;241m=\u001b[39m pre_processing()\n\u001b[0;32m     13\u001b[0m mae,mse,sqr \u001b[38;5;241m=\u001b[39m forecasting1(df_item, df_emi, emission_list)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pre_process_df' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.tsa.api as smt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.warn('DelftStack')\n",
    "warnings.warn('Do not show this message')\n",
    "\n",
    "display(pre_process_df.head(5))\n",
    "df_item, df_emi, emission_list = pre_processing()\n",
    "mae,mse,sqr = forecasting1(df_item, df_emi, emission_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2497f657",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Evaluation metrics\n",
    "#mae,mse,sqr = forecasting(df_item, df_emi, emission_list)\n",
    "average_mean_absolute_error = (sum(mae)/len(mae))*100\n",
    "average_mean_squared_error = (sum(mse)/len(mse))*100\n",
    "average_sqrt = (sum(sqr)/len(sqr))*100\n",
    "print('Mean absolute error:', average_mean_absolute_error)\n",
    "print('Mean squared error:', average_mean_squared_error)\n",
    "print('Root mean squared error:', average_sqrt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6960f3e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
